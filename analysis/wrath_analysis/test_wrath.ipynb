{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "import json\nimport re\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.stem import WordNetLemmatizer\nimport string\nimport couchdb\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\ndef MaxRepeatingCharCount(str):\n    n \u003d len(str)\n    count \u003d 0\n    cur_count \u003d 1\n\n    # Traverse string except\n    # last character\n    for i in range(n):\n        # If current character\n        # matches with next\n        if (i \u003c n - 1 and str[i] \u003d\u003d str[i + 1]):\n            cur_count +\u003d 1\n\n        # If doesn\u0027t match, update result\n        # (if required) and reset count\n        else:\n            if cur_count \u003e count:\n                count \u003d cur_count\n            cur_count \u003d 1\n    return count\n\ndef CleanTweetList(tweetlist):\n    TweetTokenList \u003d []\n    for text in tweetlist:\n        text \u003d text.translate(str.maketrans(\u0027\u0027, \u0027\u0027, string.punctuation))\n        tweetWords \u003d WordPunctTokenizer().tokenize(text.lower())\n        # remove punctuations and stop words\n        english_stops \u003d set(nltk.corpus.stopwords.words(\u0027english\u0027))\n        characters_to_remove \u003d [\"\u0027\u0027\", \u0027``\u0027, \"rt\", \"https\", \"http\", \"’\", \"“\", \"”\", \"\\u200b\", \"--\", \"n\u0027t\", \"\u0027s\", \"...\",\n                                \"//t.c\"]\n        tweetWords \u003d [tw for tw in tweetWords if tw not in english_stops]\n        tweetWords \u003d [tw for tw in tweetWords if tw not in set(characters_to_remove)]\n        # lematize\n        wnl \u003d WordNetLemmatizer()\n        TweetTokenList.append([wnl.lemmatize(tw) for tw in tweetWords])\n    return TweetTokenList\n\ndef bag_of_features(words):\n    features \u003d dict([(word, True) for word in words])\n    ###Below will not work, need to add it on top to make sense\n    capital_word_count\u003d0\n    rep \u003d 0\n    for w in words:\n        if w.isupper():\n            capital_word_count +\u003d 1\n        m \u003d MaxRepeatingCharCount(w)\n        if m \u003e rep:\n            rep \u003d m\n    #features[capital_word_count] \u003d True  # Number of All Captalized words\n    #features[rep] \u003d True #the length of maximum repeated letter, usually angry ppl repeat\n    return features",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\ntweets \u003d []\nlabels \u003d []\nbiggestrepeatcharcount \u003d []\ntrainingdatafilepath \u003d \"/Volumes/GoogleDrive/My Drive/UniMelb/Semester_1_2019/COMP90024_CCC/Assignment/Assignment2/c3-assignment2/analysis/wrath_analysis/Dataset for Detection of Cyber-Trolls.json\"\nwith open(trainingdatafilepath) as train_file:\n    # print(\"inline\")\n    for line in train_file:\n        # print(\"read\", line)\n        entry \u003d json.loads(line)\n        # print(\"contestn\")\n        tweets.append(entry[\"content\"])\n        labels.append(entry[\"annotation\"][\"label\"][0])\n        biggestrepeatcharcount.append(MaxRepeatingCharCount(entry[\"content\"]))  # this give indication of angry person or excited if theeeeeeeey repeat chars\n\n#trainhatespeechfilepath \u003d \"/Volumes/GoogleDrive/My Drive/UniMelb/Semester_1_2019/COMP90024_CCC/Assignment/Assignment2/c3-assignment2/analysis/wrath_analysis/Twitter hate speech Dataset_train_E6oV3lV.csv\"\n#train_file \u003d pd.read_csv(trainhatespeechfilepath)\n#tweets +\u003d list(train_file[\"tweet\"])\n#labels +\u003d list(train_file[\"label\"])\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "print(len(tweets))\ncleantweetlist \u003d CleanTweetList(tweets)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "dataset \u003d zip(cleantweetlist, labels)\n#dataset \u003d zip(cleantweetlist+biggestrepeatcharcount, labels)\nprint(len(labels))\n# Create new list for modeling\nfinalData \u003d []\nfor t, l in dataset:\n    finalData.append((bag_of_features(t), l))\n\n#shuffle the data to pick training \u0026 test set\n#import random\n#random.shuffle(finalData)\n#print(len(finalData))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "#used initially for testing performance\n#train_set, test_set \u003d finalData[0:14000], finalData[14000:]\n\n#refsets \u003d collections.defaultdict(set)\n#testsets \u003d collections.defaultdict(set)\n\n#nb_classifier \u003d nltk.NaiveBayesClassifier.train(train_set)\n\n#For testing only not to be used in final\n#for i, (feats, label) in enumerate(test_set):\n#    refsets[label].add(i)\n#    observed \u003d nb_classifier.classify(feats)\n#    testsets[observed].add(i)\n\n#from nltk.metrics.scores import (accuracy, precision, recall, f_measure)\n#print(\"Accuracy:\", nltk.classify.accuracy(nb_classifier, test_set))\n#print(\u0027Aggressive recall:\u0027, recall(testsets[\u00271\u0027], refsets[\u00271\u0027]))\n#print(\u0027Non-Aggressive recall:\u0027, recall(testsets[\u00270\u0027], refsets[\u00270\u0027]))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "#pure bag of words\n#Accuracy: 0.7440426595567405\n#Aggressive recall: 0.6244700181708056\n#Non-Aggressive recall: 0.890329751759911\n\n#After adding the cap letter count\n#Accuracy: 0.7382102982836194\n#Aggressive recall: 0.6175863086456181\n#Non-Aggressive recall: 0.8947166921898928\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "train_set \u003d finalData\nnb_classifier \u003d nltk.NaiveBayesClassifier.train(train_set)\n\nprint(nb_classifier.show_most_informative_features(n\u003d10))\n\n# %%\ntest_data \u003d bag_of_features(CleanTweetList([\"This is mean\"])[0])\nprint(test_data)\nprint(nb_classifier.classify(test_data))\n\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n# Database Configuration\nCOUCH_IP \u003d \u0027172.26.38.57\u0027\nBASE_URL \u003d \u0027http://172.26.38.57:5984\u0027 #to be dynamically set\nUSERNAME \u003d \u0027admin\u0027\nPASSWORD \u003d \u0027password\u0027\n#couch \u003d couchdb.Server(BASE_URL)\ncouch \u003d couchdb.Server(\"http://%s:%s@%s:5984/\" % (USERNAME, PASSWORD, COUCH_IP))\n\nsindbname \u003d \u0027tweetsins\u0027\n\nif sindbname in couch:\n    sindb \u003d couch[sindbname]\nelse:\n    sindb \u003d couch.create(sindbname)\n\nvicdb \u003d couch[\u0027victoria\u0027]\nusertimeline \u003d couch[\u0027usertimeline\u0027]\n\nvicview \u003d vicdb.view(\u0027_design/uniqueUserWithCoords/_view/uniqueUserWithCoordsDoc\u0027)\ntimelinedocs \u003d usertimeline.view(\u0027_all_docs\u0027, include_docs\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "for row in vicview.rows:\n    dic \u003d row[\"value\"]\n    if dic[\"id_str\"] not in sindb:  # Insert new records only\n        testdata \u003d bag_of_features(CleanTweetList([dic[\"text\"]])[0])\n        wrathflag \u003d nb_classifier.classify(testdata)\n        dic[\"wrathflag\"] \u003d wrathflag\n        if \"_rev\" in dic:\n            dic.pop(\"_rev\")\n        if \"_id\" in dic:\n            dic.pop(\"_id\")\n        sindb[dic[\"id_str\"]] \u003d dic\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "for row in timelinedocs.rows:\n    dic \u003d row[\"doc\"]\n    if dic[\"id_str\"] not in sindb:\n        print(\u0027data adding\u0027 + str(dic[\"id\"]))\n        testdata \u003d bag_of_features(CleanTweetList([dic[\"text\"]])[0])\n        wrathflag \u003d nb_classifier.classify(testdata)\n        dic[\"wrathflag\"] \u003d wrathflag\n        if \"_rev\" in dic:\n            dic.pop(\"_rev\")\n        if \"_id\" in dic:\n            dic.pop(\"_id\")\n        sindb[dic[\"id_str\"]] \u003d dic\n\n\n#map_fun \u003d \u0027\u0027\u0027function(doc) {\n#     if (doc.wrathflag \u003d\u003d 0)\n#         emit(doc.user, doc.text);\n# }\u0027\u0027\u0027\n#for row in sindb.query(map_fun):\n#    print(row.key)\n\n#for row in sindb.query(map_fun, descending\u003dTrue):\n#    print(row.key)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}